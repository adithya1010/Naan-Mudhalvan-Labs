{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWekLf5MZxuFHj1WYKZItU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adithya1010/Naan-Mudhalvan-Labs/blob/main/Reinforcement-Learning/RL_Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reinforcement Learning Intro:**\n",
        "\n",
        "Done with inputs from Gemini 2.0 Flash\n",
        "\n",
        "**Links to Chats:**\n",
        "\n",
        "1. https://g.co/gemini/share/632437b0bab4\n",
        "2. https://extension.getmerlin.in/chat/share/a7bbbd23-42e1-402e-b294-70353b022b1d\n"
      ],
      "metadata": {
        "id": "cPb9pHDdFZrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References:**\n",
        "\n",
        "1.https://www.geeksforgeeks.org/what-is-reinforcement-learning/\n",
        "\n",
        "2.https://www.imperial.ac.uk/media/imperial-college/faculty-of-engineering/computing/public/2021-ug-projects/State-space-decomposition-for-Reinforcement-Learning.pdf\n",
        "\n",
        "3.https://medium.com/@walkerastro41/action-space-state-space-observation-space-demystified-6c9c00a355b4"
      ],
      "metadata": {
        "id": "h5NZwTfEEUq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing libraries"
      ],
      "metadata": {
        "id": "m05W42RWXAXz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvOJ_oeK4vHh"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress specific deprecation warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "N-3MaEAnXGxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the environment with render mode specified\n",
        "env = gym.make('CartPole-v1', render_mode=\"human\")"
      ],
      "metadata": {
        "id": "jj9XFsZWXLRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the environment to get the initial state\n",
        "state = env.reset()\n"
      ],
      "metadata": {
        "id": "M8YTUd1nXShD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In reinforcement learning, the state space encompasses all possible situations an agent can find itself in, while the action space represents all the valid moves or choices an agent can make within that environment .\n",
        "\n",
        "Here's a more detailed explanation:\n",
        "\n",
        "State Space: The state space is the set of all possible configurations of the environment . A state should provide enough information to predict future rewards and states accurately, adhering to the Markov Property . In simpler terms, the state should contain all the relevant information to make a decision without needing to refer to the history of previous states or actions . The representation of the state is a crucial aspect of reinforcement learning . For complex problems, states are often represented as a vector of features . For example, in robotics, this could include positions, angles, and velocities of mechanical parts .\n",
        "\n",
        "Action Space: The action space is the set of all possible actions that the agent can take in a given state . Like state spaces, action spaces can be discrete or continuous . A discrete action space means the agent can only choose from a finite set of actions . A continuous action space means the agent can select actions from a continuous range of values .\n",
        "\n",
        "Key Considerations:\n",
        "\n",
        "Observation vs. State: It's important to distinguish between observation and state . An observation is the data an agent perceives directly from the environment, while the state is a processed representation that should satisfy the Markov Property .\n",
        "Curse of Dimensionality: In complex problems, the state space can become very large, leading to the \"curse of dimensionality\" . This makes it difficult to explore and learn effectively.\n",
        "Simplifying State Spaces: If possible, simplifying the problem to have a smaller, discrete state space (less than a few million states) can allow the use of tabular learning algorithms, which offer theoretical guarantees of convergence .\n",
        "Function Approximation: When dealing with large state spaces, function approximation techniques (such as neural networks) are used to estimate values . Feature engineering becomes important in these cases .\n",
        "Gym Standard: It is beneficial to adhere to the Gym standard for defining action spaces to ensure compatibility with various reinforcement learning algorithms .\n",
        "In essence, defining the state and action spaces correctly is fundamental to designing a reinforcement learning solution . The agent uses these spaces to learn an optimal policy, which maps states to actions to maximize rewards over time"
      ],
      "metadata": {
        "id": "j6ZmaNvGFK_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the state space and action space\n",
        "print(\"State space:\", env.observation_space)\n",
        "print(\"Action space:\", env.action_space)"
      ],
      "metadata": {
        "id": "DHclCTdgXT5Z",
        "outputId": "7eb00c1e-8308-4882-a2e3-fec6dec8bb05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
            "Action space: Discrete(2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet handles the unpacking of results from an environment's step() function, addressing the variation in return values between different environment versions or types. Here's a breakdown:\n",
        "\n",
        "Purpose:\n",
        "\n",
        "The primary goal is to reliably extract the essential information from the environment's step() output, regardless of whether the environment returns 4 or 5 values.\n",
        "\n",
        "Code Explanation:\n",
        "\n",
        "if len(step_result) == 4:\n",
        "\n",
        "This checks if the step_result tuple contains 4 elements. This usually indicates an older OpenAI Gym environment or a similar environment that doesn't explicitly return a truncated flag.\n",
        "next_state, reward, done, info = step_result: If the length is 4, it unpacks the tuple into the variables:\n",
        "next_state: The new state of the environment after the action.\n",
        "reward: The reward received for taking the action.\n",
        "done: A boolean indicating1 whether the episode has terminated2 (reached a terminal state).\n",
        "1.\n",
        "github.com\n",
        "github.com\n",
        "2.\n",
        "github.com\n",
        "github.com\n",
        "info: A dictionary containing additional information (e.g., debugging information).\n",
        "terminated = False : The terminated variable is set to false because the truncated variable is not available, so only the done variable will be used to end the episode.\n",
        "else:\n",
        "\n",
        "This handles the case where step_result contains 5 elements, which is typical for newer OpenAI Gym environments that include a truncated flag.\n",
        "next_state, reward, done, truncated, info = step_result: The tuple is unpacked into:\n",
        "next_state, reward, done, and info (same as above).\n",
        "truncated: A boolean indicating whether the episode was truncated due to a time limit or other conditions.\n",
        "terminated = done or truncated: The terminated variable is set to True if either done or truncated is True, signifying the end of the episode.\n",
        "print(f\"Action: {action}, Reward: {reward}, Next State: {next_state}, Done: {done}, Info: {info}\")\n",
        "\n",
        "This line prints the key information obtained from the step() function, providing a trace of the agent's interaction with the environment.\n",
        "if terminated:\n",
        "\n",
        "This condition checks if the episode has ended (either done or truncated is True).\n",
        "state = env.reset(): If the episode is finished, the environment is reset to its initial state, preparing for a new episode.\n",
        "env.close()\n",
        "\n",
        "This line closes the environment after all episodes are completed, releasing any resources it was using."
      ],
      "metadata": {
        "id": "0M26b8eiBRKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a few steps in the environment with random actions\n",
        "for _ in range(10):\n",
        "    env.render()  # Render the environment for visualization\n",
        "    action = env.action_space.sample()  # Take a random action\n",
        "\n",
        "    # Take a step in the environment\n",
        "    step_result = env.step(action)\n",
        "\n",
        "    # Check the number of values returned and unpack accordingly\n",
        "    if len(step_result) == 4:\n",
        "        next_state, reward, done, info = step_result\n",
        "        terminated = False\n",
        "    else:\n",
        "        next_state, reward, done, truncated, info = step_result\n",
        "        terminated = done or truncated\n",
        "\n",
        "    print(f\"Action: {action}, Reward: {reward}, Next State: {next_state}, Done: {done}, Info: {info}\")\n",
        "\n",
        "    if terminated:\n",
        "        state = env.reset()  # Reset the environment if the episode is finished\n",
        "\n",
        "env.close()  # Close the environment when done"
      ],
      "metadata": {
        "id": "LBdXTCi4Mrtt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cbf7d47-87b0-4c8f-af89-81ff4bdaeea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action: 0, Reward: 1.0, Next State: [ 0.01550099 -0.03892491 -0.04219836 -0.03132464], Done: False, Info: {}\n",
            "Action: 0, Reward: 1.0, Next State: [ 0.01472249 -0.23341711 -0.04282485  0.24775131], Done: False, Info: {}\n",
            "Action: 0, Reward: 1.0, Next State: [ 0.01005415 -0.42790213 -0.03786983  0.5266247 ], Done: False, Info: {}\n",
            "Action: 0, Reward: 1.0, Next State: [ 0.0014961  -0.62247133 -0.02733733  0.8071382 ], Done: False, Info: {}\n",
            "Action: 0, Reward: 1.0, Next State: [-0.01095332 -0.8172081  -0.01119457  1.0910981 ], Done: False, Info: {}\n",
            "Action: 0, Reward: 1.0, Next State: [-0.02729749 -1.0121808   0.01062739  1.3802476 ], Done: False, Info: {}\n",
            "Action: 0, Reward: 1.0, Next State: [-0.0475411  -1.2074337   0.03823234  1.676235  ], Done: False, Info: {}\n",
            "Action: 0, Reward: 1.0, Next State: [-0.07168978 -1.4029778   0.07175704  1.9805743 ], Done: False, Info: {}\n",
            "Action: 1, Reward: 1.0, Next State: [-0.09974933 -1.2086802   0.11136852  1.7109562 ], Done: False, Info: {}\n",
            "Action: 1, Reward: 1.0, Next State: [-0.12392294 -1.0149999   0.14558765  1.4549102 ], Done: False, Info: {}\n"
          ]
        }
      ]
    }
  ]
}